use std::time::Duration;
use serde::{de::DeserializeOwned, Serialize, Deserialize};
use anyhow::{Result, anyhow, Context};
use reqwest::Client;
use log::error;

/// OpenAI client for interacting with OpenAI API
pub struct OpenAI {
    /// HTTP client for API requests
    client: Client,
    /// API key for authentication
    api_key: String,
    /// API endpoint URL
    endpoint: String,
    /// Maximum number of retry attempts
    max_retries: u32,
    /// Base backoff time in milliseconds for exponential backoff
    backoff_base_ms: u64,
    /// Optional rate limit in requests per minute
    rate_limit: Option<u32>,
}

/// OpenAI chat completion request
#[derive(Debug, Serialize)]
pub struct OpenAIRequest {
    /// The model to use for completion
    model: String,
    
    /// The messages for the conversation
    messages: Vec<OpenAIMessage>,
    
    /// Temperature for generation (between 0 and 2)
    #[serde(skip_serializing_if = "Option::is_none")]
    temperature: Option<f32>,
    
    /// The maximum number of tokens to generate
    #[serde(skip_serializing_if = "Option::is_none")]
    max_tokens: Option<u32>,
    
    /// Top probability mass to consider (nucleus sampling)
    #[serde(skip_serializing_if = "Option::is_none")]
    top_p: Option<f32>,
    
    /// Penalizes token repetition
    #[serde(skip_serializing_if = "Option::is_none")]
    frequency_penalty: Option<f32>,
    
    /// Penalizes tokens based on appearance in text so far
    #[serde(skip_serializing_if = "Option::is_none")]
    presence_penalty: Option<f32>,
    
    /// Response format (set to "json_object" if JSON output is needed)
    #[serde(skip_serializing_if = "Option::is_none")]
    response_format: Option<OpenAIResponseFormat>,
    
    /// Unique user identifier for tracking usage
    #[serde(skip_serializing_if = "Option::is_none")]
    user: Option<String>,
}

/// Response format specification
#[derive(Debug, Serialize)]
pub struct OpenAIResponseFormat {
    /// The type of response format
    #[serde(rename = "type")]
    format_type: String,
}

/// OpenAI message format
#[derive(Debug, Serialize, Deserialize)]
pub struct OpenAIMessage {
    /// Role of the message sender (system, user, assistant)
    pub role: String,
    
    /// Content of the message
    pub content: String,
}

/// Token usage information
#[derive(Debug, Deserialize)]
pub struct TokenUsage {
    /// Number of prompt tokens
    pub prompt_tokens: u32,
    /// Number of completion tokens
    pub completion_tokens: u32,
    /// Total number of tokens - required by API response format
    #[allow(dead_code)]
    pub total_tokens: u32,
}

impl AsRef<TokenUsage> for TokenUsage {
    fn as_ref(&self) -> &TokenUsage {
        self
    }
}

/// OpenAI completion response
#[derive(Debug, Deserialize)]
pub struct OpenAIResponse {
    /// The choices generated by the model
    pub choices: Vec<OpenAIChoice>,
    /// Token usage information (optional on some OpenAI-compatible servers)
    pub usage: Option<TokenUsage>,
}

/// Individual choice in an OpenAI response
#[derive(Debug, Deserialize)]
pub struct OpenAIChoice {
    /// The message content
    pub message: OpenAIMessage,
}

impl Default for OpenAIRequest {
    fn default() -> Self {
        Self {
            model: String::new(),
            messages: Vec::new(),
            temperature: Some(0.7),
            max_tokens: Some(4096),
            top_p: None,
            frequency_penalty: None,
            presence_penalty: None,
            response_format: None,
            user: None,
        }
    }
}

/// Builder methods for OpenAIRequest - API surface for library consumers
#[allow(dead_code)]
impl OpenAIRequest {
    /// Create a new OpenAI request
    pub fn new(model: impl Into<String>) -> Self {
        Self {
            model: model.into(),
            ..Default::default()
        }
    }
    
    /// Add a message to the request
    pub fn add_message(mut self, role: impl Into<String>, content: impl Into<String>) -> Self {
        self.messages.push(OpenAIMessage {
            role: role.into(),
            content: content.into(),
        });
        self
    }
    
    /// Set the temperature
    pub fn temperature(mut self, temperature: f32) -> Self {
        self.temperature = Some(temperature);
        self
    }
    
    /// Set the maximum number of tokens
    pub fn max_tokens(mut self, max_tokens: u32) -> Self {
        self.max_tokens = Some(max_tokens);
        self
    }
    
    /// Set the top_p (nucleus sampling)
    pub fn top_p(mut self, top_p: f32) -> Self {
        self.top_p = Some(top_p);
        self
    }
    
    /// Set the frequency penalty
    pub fn frequency_penalty(mut self, frequency_penalty: f32) -> Self {
        self.frequency_penalty = Some(frequency_penalty);
        self
    }
    
    /// Set the presence penalty
    pub fn presence_penalty(mut self, presence_penalty: f32) -> Self {
        self.presence_penalty = Some(presence_penalty);
        self
    }
    
    /// Set the response format to JSON
    pub fn json_response_format(mut self) -> Self {
        self.response_format = Some(OpenAIResponseFormat {
            format_type: "json_object".to_string(),
        });
        self
    }
    
    /// Set the user identifier
    pub fn user(mut self, user: impl Into<String>) -> Self {
        self.user = Some(user.into());
        self
    }
}

/// OpenAI client implementation - some methods are API surface for library consumers
#[allow(dead_code)]
impl OpenAI {
    /// Create a new OpenAI client
    pub fn new(api_key: impl Into<String>, endpoint: impl Into<String>) -> Self {
        Self {
            client: Client::builder()
                .timeout(Duration::from_secs(60))
                .build()
                .unwrap_or_default(),
            api_key: api_key.into(),
            endpoint: endpoint.into(),
            max_retries: 3,
            backoff_base_ms: 1000,
            rate_limit: None,
        }
    }
    
    /// Create a new OpenAI client with configuration
    /// 
    /// Uses connection pooling for better performance with concurrent requests.
    /// Note: We force HTTP/1.1 since many OpenAI-compatible servers (like LM Studio) don't support HTTP/2.
    pub fn new_with_config(
        api_key: impl Into<String>, 
        endpoint: impl Into<String>,
        max_retries: u32,
        backoff_base_ms: u64,
        rate_limit: Option<u32>
    ) -> Self {
        Self {
            client: Client::builder()
                .timeout(Duration::from_secs(120))
                // Force HTTP/1.1 only - many local servers don't support HTTP/2
                .http1_only()
                // Keep connections alive for better performance with concurrent requests
                .pool_idle_timeout(Duration::from_secs(90))
                .pool_max_idle_per_host(20)  // Allow more connections for parallel requests
                // Enable TCP keepalive
                .tcp_keepalive(Duration::from_secs(60))
                .build()
                .unwrap_or_default(),
            api_key: api_key.into(),
            endpoint: endpoint.into(),
            max_retries,
            backoff_base_ms,
            rate_limit,
        }
    }
    
    /// Complete a chat request with retry logic
    pub async fn complete(&self, request: OpenAIRequest) -> Result<OpenAIResponse> {
        let api_url = if self.endpoint.is_empty() {
            "https://api.openai.com/v1/chat/completions".to_string()
        } else {
            format!("{}/chat/completions", self.endpoint.trim_end_matches('/'))
        };
        
        let mut attempt = 0;
        let mut last_error = None;
        
        while attempt <= self.max_retries {
            // Add rate limiting if configured
            if let Some(rate_limit) = self.rate_limit {
                let delay_ms = 60_000 / rate_limit as u64; // Convert requests per minute to milliseconds
                if attempt > 0 {
                    tokio::time::sleep(Duration::from_millis(delay_ms)).await;
                }
            }
            
            // Add timeout to prevent hanging HTTP requests
            let request_future = self.client.post(&api_url)
                .header("Content-Type", "application/json")
                .header("Authorization", format!("Bearer {}", self.api_key))
                .json(&request)
                .send();
            
            let timeout_duration = Duration::from_secs(60); // 1 minute timeout
            let response_result = tokio::select! {
                result = request_future => {
                    result.map_err(|e| anyhow!("Failed to send request to OpenAI API: {}", e))
                },
                _ = tokio::time::sleep(timeout_duration) => {
                    Err(anyhow!("OpenAI API request timed out after 60 seconds"))
                }
            };
            
            match response_result {
                Ok(response) => {
                    let status = response.status();
                    if status.is_success() {
                        // Success - parse and return response
                        let openai_response = response.json::<OpenAIResponse>().await
                            .with_context(|| "Failed to parse OpenAI API response")?;
                        return Ok(openai_response);
                    } else if status.as_u16() == 429 || status.is_server_error() {
                        // Rate limit or server error - can retry
                        let error_text = response.text().await
                            .unwrap_or_else(|_| "Failed to get error response text".to_string());
                        last_error = Some(anyhow!("OpenAI API error ({}): {}", status, error_text));
                        error!("OpenAI API error ({}): {} - attempt {}/{}", status, error_text, attempt + 1, self.max_retries + 1);
                    } else {
                        // Client error (4xx, but not 429) - don't retry
                        let error_text = response.text().await
                            .unwrap_or_else(|_| "Failed to get error response text".to_string());
                        error!("OpenAI API error ({}): {}", status, error_text);
                        return Err(anyhow!("OpenAI API error ({}): {}", status, error_text));
                    }
                },
                Err(e) => {
                    // Network error - can retry
                    last_error = Some(e);
                    error!("OpenAI API network error: {} - attempt {}/{}", last_error.as_ref().unwrap(), attempt + 1, self.max_retries + 1);
                }
            }
            
            attempt += 1;
            
            // If we have more retries left, wait with exponential backoff
            if attempt <= self.max_retries {
                let backoff_ms = self.backoff_base_ms * (1u64 << (attempt - 1));
                tokio::time::sleep(Duration::from_millis(backoff_ms)).await;
            }
        }
        
        // If we get here, all retries failed
        Err(last_error.unwrap_or_else(|| anyhow!("OpenAI API request failed after {} attempts", self.max_retries + 1)))
    }

    /// Complete a chat request with JSON mode and parse the response.
    ///
    /// This method enables JSON mode in the request and parses the response
    /// into the specified type. The system message should instruct the model
    /// to output valid JSON matching the expected schema.
    ///
    /// # Type Parameters
    /// * `T` - The type to deserialize the JSON response into
    ///
    /// # Arguments
    /// * `model` - The model to use
    /// * `system_prompt` - System message instructing JSON output format
    /// * `user_prompt` - The user message (typically containing the data to process)
    /// * `temperature` - Temperature for generation (0.0-2.0)
    /// * `max_tokens` - Maximum tokens to generate
    ///
    /// # Returns
    /// The parsed response of type T, or an error if parsing fails
    pub async fn complete_json<T: DeserializeOwned>(
        &self,
        model: &str,
        system_prompt: &str,
        user_prompt: &str,
        temperature: f32,
        max_tokens: u32,
    ) -> Result<T> {
        let request = OpenAIRequest::new(model)
            .add_message("system", system_prompt)
            .add_message("user", user_prompt)
            .temperature(temperature)
            .max_tokens(max_tokens)
            .json_response_format();

        let response = self.complete(request).await?;

        // Extract the content from the response
        let content = response
            .choices
            .first()
            .map(|choice| choice.message.content.as_str())
            .ok_or_else(|| anyhow!("Empty response from OpenAI API"))?;

        // Parse the JSON response
        serde_json::from_str(content)
            .with_context(|| format!("Failed to parse JSON response: {}", content))
    }
} 