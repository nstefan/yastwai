use std::time::Duration;
use serde::{Serialize, Deserialize};
use anyhow::{Result, anyhow, Context};
use reqwest::Client;
use log::{error, info};

/// OpenAI client for interacting with OpenAI API
pub struct OpenAI {
    /// HTTP client for API requests
    client: Client,
    /// API key for authentication
    api_key: String,
    /// API endpoint URL
    endpoint: String,
}

/// OpenAI chat completion request
#[derive(Debug, Serialize)]
pub struct OpenAIRequest {
    /// The model to use for completion
    model: String,
    
    /// The messages for the conversation
    messages: Vec<OpenAIMessage>,
    
    /// Temperature for generation (between 0 and 2)
    #[serde(skip_serializing_if = "Option::is_none")]
    temperature: Option<f32>,
    
    /// The maximum number of tokens to generate
    #[serde(skip_serializing_if = "Option::is_none")]
    max_tokens: Option<u32>,
    
    /// Top probability mass to consider (nucleus sampling)
    #[serde(skip_serializing_if = "Option::is_none")]
    top_p: Option<f32>,
    
    /// Penalizes token repetition
    #[serde(skip_serializing_if = "Option::is_none")]
    frequency_penalty: Option<f32>,
    
    /// Penalizes tokens based on appearance in text so far
    #[serde(skip_serializing_if = "Option::is_none")]
    presence_penalty: Option<f32>,
    
    /// Response format (set to "json_object" if JSON output is needed)
    #[serde(skip_serializing_if = "Option::is_none")]
    response_format: Option<OpenAIResponseFormat>,
    
    /// Unique user identifier for tracking usage
    #[serde(skip_serializing_if = "Option::is_none")]
    user: Option<String>,
}

/// Response format specification
#[derive(Debug, Serialize)]
pub struct OpenAIResponseFormat {
    /// The type of response format
    #[serde(rename = "type")]
    format_type: String,
}

/// OpenAI message format
#[derive(Debug, Serialize, Deserialize)]
pub struct OpenAIMessage {
    /// Role of the message sender (system, user, assistant)
    pub role: String,
    
    /// Content of the message
    pub content: String,
}

/// Token usage information
#[derive(Debug, Deserialize)]
pub struct TokenUsage {
    /// Number of prompt tokens
    pub prompt_tokens: u32,
    /// Number of completion tokens
    pub completion_tokens: u32,
    /// Total number of tokens
    pub total_tokens: u32,
}

/// OpenAI completion response
#[derive(Debug, Deserialize)]
pub struct OpenAIResponse {
    /// The choices generated by the model
    pub choices: Vec<OpenAIChoice>,
    /// Token usage information
    pub usage: TokenUsage,
}

/// Individual choice in an OpenAI response
#[derive(Debug, Deserialize)]
pub struct OpenAIChoice {
    /// The message content
    pub message: OpenAIMessage,
}

impl Default for OpenAIRequest {
    fn default() -> Self {
        Self {
            model: String::new(),
            messages: Vec::new(),
            temperature: Some(0.7),
            max_tokens: Some(4096),
            top_p: None,
            frequency_penalty: None,
            presence_penalty: None,
            response_format: None,
            user: None,
        }
    }
}

impl OpenAIRequest {
    /// Create a new OpenAI request
    pub fn new(model: impl Into<String>) -> Self {
        Self {
            model: model.into(),
            ..Default::default()
        }
    }
    
    /// Add a message to the request
    pub fn add_message(mut self, role: impl Into<String>, content: impl Into<String>) -> Self {
        self.messages.push(OpenAIMessage {
            role: role.into(),
            content: content.into(),
        });
        self
    }
    
    /// Set the temperature
    pub fn temperature(mut self, temperature: f32) -> Self {
        self.temperature = Some(temperature);
        self
    }
    
    /// Set the maximum number of tokens
    pub fn max_tokens(mut self, max_tokens: u32) -> Self {
        self.max_tokens = Some(max_tokens);
        self
    }
    
    /// Set the top_p (nucleus sampling)
    pub fn top_p(mut self, top_p: f32) -> Self {
        self.top_p = Some(top_p);
        self
    }
    
    /// Set the frequency penalty
    pub fn frequency_penalty(mut self, frequency_penalty: f32) -> Self {
        self.frequency_penalty = Some(frequency_penalty);
        self
    }
    
    /// Set the presence penalty
    pub fn presence_penalty(mut self, presence_penalty: f32) -> Self {
        self.presence_penalty = Some(presence_penalty);
        self
    }
    
    /// Set the response format to JSON
    pub fn json_response_format(mut self) -> Self {
        self.response_format = Some(OpenAIResponseFormat {
            format_type: "json_object".to_string(),
        });
        self
    }
    
    /// Set the user identifier
    pub fn user(mut self, user: impl Into<String>) -> Self {
        self.user = Some(user.into());
        self
    }
}

impl OpenAI {
    /// Create a new OpenAI client
    pub fn new(api_key: impl Into<String>, endpoint: impl Into<String>) -> Self {
        Self {
            client: Client::builder()
                .timeout(Duration::from_secs(60))
                .build()
                .unwrap_or_default(),
            api_key: api_key.into(),
            endpoint: endpoint.into(),
        }
    }
    
    /// Complete a chat request
    pub async fn complete(&self, request: OpenAIRequest) -> Result<OpenAIResponse> {
        let api_url = if self.endpoint.is_empty() {
            "https://api.openai.com/v1/chat/completions".to_string()
        } else {
            format!("{}/chat/completions", self.endpoint.trim_end_matches('/'))
        };
        
        let response = self.client.post(&api_url)
            .header("Content-Type", "application/json")
            .header("Authorization", format!("Bearer {}", self.api_key))
            .json(&request)
            .send()
            .await
            .map_err(|e| anyhow!("Failed to send request to OpenAI API: {}", e))?;
        
        let status = response.status();
        if !status.is_success() {
            let error_text = response.text().await
                .unwrap_or_else(|_| "Failed to get error response text".to_string());
            error!("OpenAI API error ({}): {}", status, error_text);
            return Err(anyhow!("OpenAI API error ({}): {}", status, error_text));
        }
        
        let openai_response = response.json::<OpenAIResponse>().await
            .with_context(|| "Failed to parse OpenAI API response")?;
        
        Ok(openai_response)
    }
    
    /// Test the connection to the OpenAI API
    pub async fn test_connection(&self, model: &str) -> Result<()> {
        let request = OpenAIRequest::new(model)
            .add_message("user", "Hello")
            .max_tokens(10);
        
        self.complete(request).await?;
        Ok(())
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    
    #[tokio::test]
    async fn test_openai_complete() {
        // This test should only run if an API key is provided
        let api_key = std::env::var("OPENAI_API_KEY").unwrap_or_default();
        if api_key.is_empty() {
            return;
        }
        
        let client = OpenAI::new(api_key, "");
        let request = OpenAIRequest::new("gpt-3.5-turbo")
            .add_message("system", "You are a helpful assistant.")
            .add_message("user", "Say hello!")
            .max_tokens(10);
        
        let response = client.complete(request).await.unwrap();
        assert!(!response.choices.is_empty());
        assert!(!response.choices[0].message.content.is_empty());
    }
} 