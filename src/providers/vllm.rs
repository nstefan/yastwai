use std::time::Duration;
use serde::{de::DeserializeOwned, Serialize, Deserialize};
use anyhow::{Result, anyhow, Context};
use reqwest::Client;
use log::{error, debug};

/// vLLM client for interacting with vLLM's OpenAI-compatible API
///
/// vLLM provides high-throughput LLM serving with features like:
/// - Continuous batching (PagedAttention)
/// - High concurrency support
/// - No rate limiting (local deployment)
pub struct VLLM {
    /// HTTP client for API requests
    client: Client,
    /// API key for authentication (optional for some deployments)
    api_key: Option<String>,
    /// API endpoint URL
    endpoint: String,
    /// Maximum number of retry attempts
    max_retries: u32,
    /// Base backoff time in milliseconds for exponential backoff
    backoff_base_ms: u64,
}

/// vLLM chat completion request (OpenAI-compatible format)
#[derive(Debug, Serialize)]
pub struct VLLMRequest {
    /// The model to use for completion
    model: String,

    /// The messages for the conversation
    messages: Vec<VLLMMessage>,

    /// Temperature for generation (between 0 and 2)
    #[serde(skip_serializing_if = "Option::is_none")]
    temperature: Option<f32>,

    /// The maximum number of tokens to generate
    #[serde(skip_serializing_if = "Option::is_none")]
    max_tokens: Option<u32>,

    /// Top probability mass to consider (nucleus sampling)
    #[serde(skip_serializing_if = "Option::is_none")]
    top_p: Option<f32>,

    /// Penalizes token repetition
    #[serde(skip_serializing_if = "Option::is_none")]
    frequency_penalty: Option<f32>,

    /// Penalizes tokens based on appearance in text so far
    #[serde(skip_serializing_if = "Option::is_none")]
    presence_penalty: Option<f32>,

    /// Response format (set to "json_object" if JSON output is needed)
    #[serde(skip_serializing_if = "Option::is_none")]
    response_format: Option<VLLMResponseFormat>,

    /// Whether to stream the response
    #[serde(skip_serializing_if = "Option::is_none")]
    stream: Option<bool>,

    /// vLLM-specific: best_of parameter for beam search
    #[serde(skip_serializing_if = "Option::is_none")]
    best_of: Option<u32>,

    /// vLLM-specific: use beam search
    #[serde(skip_serializing_if = "Option::is_none")]
    use_beam_search: Option<bool>,
}

/// Response format specification
#[derive(Debug, Serialize)]
pub struct VLLMResponseFormat {
    /// The type of response format
    #[serde(rename = "type")]
    format_type: String,
}

/// vLLM message format
#[derive(Debug, Serialize, Deserialize)]
pub struct VLLMMessage {
    /// Role of the message sender (system, user, assistant)
    pub role: String,

    /// Content of the message
    pub content: String,
}

/// Token usage information
#[derive(Debug, Deserialize)]
pub struct TokenUsage {
    /// Number of prompt tokens
    pub prompt_tokens: u32,
    /// Number of completion tokens
    pub completion_tokens: u32,
    /// Total number of tokens
    #[allow(dead_code)]
    pub total_tokens: u32,
}

impl AsRef<TokenUsage> for TokenUsage {
    fn as_ref(&self) -> &TokenUsage {
        self
    }
}

/// vLLM completion response
#[derive(Debug, Deserialize)]
pub struct VLLMResponse {
    /// The choices generated by the model
    pub choices: Vec<VLLMChoice>,
    /// Token usage information
    pub usage: Option<TokenUsage>,
}

/// Individual choice in a vLLM response
#[derive(Debug, Deserialize)]
pub struct VLLMChoice {
    /// The message content
    pub message: VLLMMessage,
}

impl Default for VLLMRequest {
    fn default() -> Self {
        Self {
            model: String::new(),
            messages: Vec::new(),
            temperature: Some(0.7),
            max_tokens: Some(4096),
            top_p: None,
            frequency_penalty: None,
            presence_penalty: None,
            response_format: None,
            stream: Some(false),
            best_of: None,
            use_beam_search: None,
        }
    }
}

/// Builder methods for VLLMRequest
#[allow(dead_code)]
impl VLLMRequest {
    /// Create a new vLLM request
    pub fn new(model: impl Into<String>) -> Self {
        Self {
            model: model.into(),
            ..Default::default()
        }
    }

    /// Add a message to the request
    pub fn add_message(mut self, role: impl Into<String>, content: impl Into<String>) -> Self {
        self.messages.push(VLLMMessage {
            role: role.into(),
            content: content.into(),
        });
        self
    }

    /// Set the temperature
    pub fn temperature(mut self, temperature: f32) -> Self {
        self.temperature = Some(temperature);
        self
    }

    /// Set the maximum number of tokens
    pub fn max_tokens(mut self, max_tokens: u32) -> Self {
        self.max_tokens = Some(max_tokens);
        self
    }

    /// Set the top_p (nucleus sampling)
    pub fn top_p(mut self, top_p: f32) -> Self {
        self.top_p = Some(top_p);
        self
    }

    /// Set the frequency penalty
    pub fn frequency_penalty(mut self, frequency_penalty: f32) -> Self {
        self.frequency_penalty = Some(frequency_penalty);
        self
    }

    /// Set the presence penalty
    pub fn presence_penalty(mut self, presence_penalty: f32) -> Self {
        self.presence_penalty = Some(presence_penalty);
        self
    }

    /// Set the response format to JSON
    pub fn json_response_format(mut self) -> Self {
        self.response_format = Some(VLLMResponseFormat {
            format_type: "json_object".to_string(),
        });
        self
    }

    /// Set best_of for beam search (vLLM-specific)
    pub fn best_of(mut self, best_of: u32) -> Self {
        self.best_of = Some(best_of);
        self
    }

    /// Enable beam search (vLLM-specific)
    pub fn use_beam_search(mut self, use_beam_search: bool) -> Self {
        self.use_beam_search = Some(use_beam_search);
        self
    }
}

/// vLLM client implementation
#[allow(dead_code)]
impl VLLM {
    /// Create a new vLLM client with default settings
    pub fn new(endpoint: impl Into<String>) -> Self {
        Self {
            client: Client::builder()
                .timeout(Duration::from_secs(120))
                .build()
                .unwrap_or_default(),
            api_key: None,
            endpoint: endpoint.into(),
            max_retries: 3,
            backoff_base_ms: 1000,
        }
    }

    /// Create a new vLLM client with configuration
    ///
    /// vLLM supports high concurrency, so connection pooling is optimized accordingly.
    pub fn new_with_config(
        endpoint: impl Into<String>,
        api_key: impl Into<String>,
        max_retries: u32,
        backoff_base_ms: u64,
    ) -> Self {
        let api_key_str = api_key.into();
        let api_key = if api_key_str.is_empty() { None } else { Some(api_key_str) };

        Self {
            client: Client::builder()
                .timeout(Duration::from_secs(180)) // Longer timeout for vLLM batch processing
                // Use HTTP/1.1 - most vLLM deployments don't support HTTP/2
                .http1_only()
                // Large connection pool for high parallelism
                .pool_idle_timeout(Duration::from_secs(120))
                .pool_max_idle_per_host(32) // Higher for vLLM's high concurrency support
                // TCP keepalive
                .tcp_keepalive(Duration::from_secs(60))
                .build()
                .unwrap_or_default(),
            api_key,
            endpoint: endpoint.into(),
            max_retries,
            backoff_base_ms,
        }
    }

    /// Complete a chat request with retry logic
    pub async fn complete(&self, request: VLLMRequest) -> Result<VLLMResponse> {
        let api_url = format!("{}/chat/completions", self.endpoint.trim_end_matches('/'));

        let mut attempt = 0;
        let mut last_error = None;

        while attempt <= self.max_retries {
            let mut req_builder = self.client.post(&api_url)
                .header("Content-Type", "application/json");

            // Add Authorization header if API key is configured
            if let Some(ref api_key) = self.api_key {
                req_builder = req_builder.header("Authorization", format!("Bearer {}", api_key));
            }

            let request_future = req_builder.json(&request).send();

            let timeout_duration = Duration::from_secs(120);
            let response_result = tokio::select! {
                result = request_future => {
                    result.map_err(|e| anyhow!("Failed to send request to vLLM API: {}", e))
                },
                _ = tokio::time::sleep(timeout_duration) => {
                    Err(anyhow!("vLLM API request timed out after 120 seconds"))
                }
            };

            match response_result {
                Ok(response) => {
                    let status = response.status();
                    if status.is_success() {
                        let vllm_response = response.json::<VLLMResponse>().await
                            .with_context(|| "Failed to parse vLLM API response")?;
                        return Ok(vllm_response);
                    } else if status.is_server_error() {
                        // Server error - can retry
                        let error_text = response.text().await
                            .unwrap_or_else(|_| "Failed to get error response text".to_string());
                        last_error = Some(anyhow!("vLLM API error ({}): {}", status, error_text));
                        error!("vLLM API error ({}): {} - attempt {}/{}", status, error_text, attempt + 1, self.max_retries + 1);
                    } else {
                        // Client error (4xx) - don't retry
                        let error_text = response.text().await
                            .unwrap_or_else(|_| "Failed to get error response text".to_string());
                        error!("vLLM API error ({}): {}", status, error_text);
                        return Err(anyhow!("vLLM API error ({}): {}", status, error_text));
                    }
                },
                Err(e) => {
                    // Network error - can retry
                    last_error = Some(e);
                    error!("vLLM API network error: {} - attempt {}/{}", last_error.as_ref().unwrap(), attempt + 1, self.max_retries + 1);
                }
            }

            attempt += 1;

            // If we have more retries left, wait with exponential backoff
            if attempt <= self.max_retries {
                let backoff_ms = self.backoff_base_ms * (1u64 << (attempt - 1));
                tokio::time::sleep(Duration::from_millis(backoff_ms)).await;
            }
        }

        // If we get here, all retries failed
        Err(last_error.unwrap_or_else(|| anyhow!("vLLM API request failed after {} attempts", self.max_retries + 1)))
    }

    /// Complete a chat request with JSON mode and parse the response.
    pub async fn complete_json<T: DeserializeOwned>(
        &self,
        model: &str,
        system_prompt: &str,
        user_prompt: &str,
        temperature: f32,
        max_tokens: u32,
    ) -> Result<T> {
        let request = VLLMRequest::new(model)
            .add_message("system", system_prompt)
            .add_message("user", user_prompt)
            .temperature(temperature)
            .max_tokens(max_tokens)
            .json_response_format();

        let response = self.complete(request).await?;

        // Extract the content from the response
        let content = response
            .choices
            .first()
            .map(|choice| choice.message.content.as_str())
            .ok_or_else(|| anyhow!("Empty response from vLLM API"))?;

        // Parse the JSON response
        serde_json::from_str(content)
            .with_context(|| format!("Failed to parse JSON response: {}", content))
    }

    /// Check vLLM server health
    pub async fn health_check(&self) -> Result<()> {
        let health_url = format!("{}/health", self.endpoint.trim_end_matches('/'));

        let mut req_builder = self.client.get(&health_url)
            .timeout(Duration::from_secs(10));

        if let Some(ref api_key) = self.api_key {
            req_builder = req_builder.header("Authorization", format!("Bearer {}", api_key));
        }

        let response = req_builder.send()
            .await
            .context("Failed to connect to vLLM server")?;

        if response.status().is_success() {
            debug!("vLLM health check passed");
            Ok(())
        } else {
            Err(anyhow!("vLLM health check failed with status: {}", response.status()))
        }
    }

    /// Get list of available models from vLLM
    pub async fn list_models(&self) -> Result<Vec<String>> {
        let models_url = format!("{}/models", self.endpoint.trim_end_matches('/'));

        let mut req_builder = self.client.get(&models_url)
            .timeout(Duration::from_secs(10));

        if let Some(ref api_key) = self.api_key {
            req_builder = req_builder.header("Authorization", format!("Bearer {}", api_key));
        }

        let response = req_builder.send()
            .await
            .context("Failed to get models from vLLM")?;

        if !response.status().is_success() {
            return Err(anyhow!("Failed to list models: {}", response.status()));
        }

        #[derive(Deserialize)]
        struct ModelsResponse {
            data: Vec<ModelInfo>,
        }

        #[derive(Deserialize)]
        struct ModelInfo {
            id: String,
        }

        let models_response: ModelsResponse = response.json().await
            .context("Failed to parse models response")?;

        Ok(models_response.data.into_iter().map(|m| m.id).collect())
    }
}
