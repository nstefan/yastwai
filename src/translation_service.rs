use anyhow::{Result, Context, anyhow};
use reqwest::Client;
use serde::{Deserialize, Serialize};
use std::time::Duration;
use log::{info, error, warn};
use ollama_rs::Ollama;
use ollama_rs::generation::completion::request::GenerationRequest;
use url::Url;
use std::sync::Arc;
use regex::Regex;
use std::sync::atomic::{AtomicUsize, Ordering};
use futures::future::join_all;

use crate::app_config::{TranslationConfig, TranslationProvider as ConfigTranslationProvider};
use crate::subtitle_processor::SubtitleEntry;

/** Translation service module that implements different providers
 * (Ollama, OpenAI, Anthropic) for translating subtitle content.
 */
/// OpenAI chat completion request
#[derive(Debug, Serialize)]
struct OpenAIRequest {
    /// The model to use for completion
    model: String,
    
    /// The messages for the conversation
    messages: Vec<OpenAIMessage>,
    
    /// Temperature for generation (between 0 and 2)
    #[serde(skip_serializing_if = "Option::is_none")]
    temperature: Option<f32>,
    
    /// The maximum number of tokens to generate
    #[serde(skip_serializing_if = "Option::is_none")]
    max_tokens: Option<u32>,
    
    /// Top probability mass to consider (nucleus sampling)
    #[serde(skip_serializing_if = "Option::is_none")]
    top_p: Option<f32>,
    
    /// Penalizes token repetition
    #[serde(skip_serializing_if = "Option::is_none")]
    frequency_penalty: Option<f32>,
    
    /// Penalizes tokens based on appearance in text so far
    #[serde(skip_serializing_if = "Option::is_none")]
    presence_penalty: Option<f32>,
    
    /// Response format (set to "json_object" if JSON output is needed)
    #[serde(skip_serializing_if = "Option::is_none")]
    response_format: Option<OpenAIResponseFormat>,
    
    /// Unique user identifier for tracking usage
    #[serde(skip_serializing_if = "Option::is_none")]
    user: Option<String>,
}

/// Response format specification
#[derive(Debug, Serialize)]
struct OpenAIResponseFormat {
    /// The type of response format
    #[serde(rename = "type")]
    format_type: String,
}

impl Default for OpenAIRequest {
    fn default() -> Self {
        Self {
            model: String::new(),
            messages: Vec::new(),
            temperature: Some(0.7),
            max_tokens: Some(4096),
            top_p: None,
            frequency_penalty: None,
            presence_penalty: None,
            response_format: None,
            user: None,
        }
    }
}

/// OpenAI message format
#[derive(Debug, Serialize, Deserialize)]
struct OpenAIMessage {
    /// Role of the message sender (system, user, assistant)
    role: String,
    
    /// Content of the message
    content: String,
}

/// OpenAI completion response
#[derive(Debug, Deserialize)]
struct OpenAIResponse {
    /// The choices generated by the model
    choices: Vec<OpenAIChoice>,
}

/// Individual choice in an OpenAI response
#[derive(Debug, Deserialize)]
struct OpenAIChoice {
    /// The message content
    message: OpenAIMessage,
}

/// Anthropic message request
#[derive(Debug, Serialize)]
struct AnthropicRequest {
    /// The model to use
    model: String,
    
    /// The messages for the conversation
    messages: Vec<AnthropicMessage>,
    
    /// System prompt to guide the AI
    #[serde(skip_serializing_if = "Option::is_none")]
    system: Option<String>,
    
    /// Temperature for generation
    #[serde(skip_serializing_if = "Option::is_none")]
    temperature: Option<f32>,
    
    /// Maximum number of tokens to generate
    max_tokens: u32,
    
    /// Top probability mass to consider (nucleus sampling)
    #[serde(skip_serializing_if = "Option::is_none")]
    top_p: Option<f32>,
    
    /// Top k tokens to consider
    #[serde(skip_serializing_if = "Option::is_none")]
    top_k: Option<u32>,
}

impl Default for AnthropicRequest {
    fn default() -> Self {
        Self {
            model: String::new(),
            messages: Vec::new(),
            system: None,
            temperature: Some(0.7),
            max_tokens: 4096,
            top_p: None,
            top_k: None,
        }
    }
}

/// Anthropic message format
#[derive(Debug, Serialize)]
struct AnthropicMessage {
    /// Role of the message sender (user, assistant)
    role: String,
    
    /// Content of the message
    content: String,
}

/// Anthropic response
#[derive(Debug, Deserialize)]
struct AnthropicResponse {
    /// The content of the response
    content: Vec<AnthropicContent>,
}

/// Individual content block in an Anthropic response
#[derive(Debug, Deserialize)]
struct AnthropicContent {
    /// The type of content
    #[serde(rename = "type")]
    content_type: String,
    
    /// The actual text content
    text: String,
}

/// Parse endpoint string into host and port
fn parse_endpoint(endpoint: &str) -> Result<(String, u16)> {
    // If it doesn't start with http/https, assume it's just host:port
    let url_str = if !endpoint.starts_with("http://") && !endpoint.starts_with("https://") {
        format!("http://{}", endpoint)
    } else {
        endpoint.to_string()
    };
    
    let url = Url::parse(&url_str)
        .context(format!("Failed to parse endpoint URL: {}", endpoint))?;
        
    let host = format!(
        "{}://{}",
        url.scheme(),
        url.host_str().unwrap_or("localhost")
    );
    
    let port = url.port().unwrap_or(11434);
    
    Ok((host, port))
}

/// Concrete translation provider implementations
enum TranslationProviderImpl {    
    /// Ollama LLM based translation service
    Ollama {
        /// Ollama client
        client: Ollama,
    },
    
    /// OpenAI API based translation service
    OpenAI {
        /// HTTP client for API requests
        client: Client,
    },
    
    /// Anthropic API based translation service
    Anthropic {
        /// HTTP client for API requests
        client: Client,
    },
}

/// Translation service that manages different provider implementations
pub struct TranslationService {
    /// Provider implementation
    provider: TranslationProviderImpl,
    
    /// Translation service configuration
    config: TranslationConfig,
}

/// Extract translated text from a potentially complex API response
///
/// This function handles various response formats from translation APIs,
/// including JSON structures, delimited text blocks, and plain text.
fn extract_translated_text_from_response(response: &str) -> Result<String> {
    // If the response is empty, return an error immediately
    if response.trim().is_empty() {
        return Err(anyhow!("Empty response received from translation service"));
    }
    
    // Quick check for pure text (most common case in newer implementations)
    // This avoids unnecessary parsing when the response is already clean
    if !response.contains('{') && !response.contains('}') && 
       !response.contains("===") && !response.contains("```") {
        return Ok(response.trim().to_string());
    }
    
    // Look for the translation block between standard delimiters
    if let Some(start) = response.find("=== BEGIN TRANSLATION ===") {
        if let Some(end) = response.find("=== END TRANSLATION ===") {
            if start < end {
                let content = &response[start + "=== BEGIN TRANSLATION ===".len()..end];
                return Ok(content.trim().to_string());
            }
        }
    }
    
    // Try to find markdown code blocks (common in newer LLM responses)
    let code_block_regex = Regex::new(r"```(?:json|text)?\s*\n([\s\S]*?)\n\s*```").unwrap_or_else(|_| {
        // Fallback to a simpler pattern if the main one fails to compile
        Regex::new(r"```([\s\S]*?)```").unwrap()
    });
    
    if let Some(caps) = code_block_regex.captures(response) {
        if let Some(content) = caps.get(1) {
            return Ok(content.as_str().trim().to_string());
        }
    }
    
    // Try to extract JSON (common in structured API responses)
    let json_content = if let (Some(start), Some(end)) = (response.find('{'), response.rfind('}')) {
        if start < end {
            Some(&response[start..=end])
        } else {
            None
        }
    } else {
        None
    };
    
    if let Some(content) = json_content {
        // Try to parse as JSON and extract relevant fields
        if let Ok(json) = serde_json::from_str::<serde_json::Value>(content) {
            // Check common fields where translation content might be stored
            for field in ["text", "content", "translation", "translated_text", "result"] {
                if let Some(text) = json.get(field).and_then(|v| v.as_str()) {
                    return Ok(text.to_string());
                }
            }
            
            // Check for array of translations (common pattern in batch responses)
            for field in ["entries", "translations", "results", "items"] {
                if let Some(entries) = json.get(field).and_then(|v| v.as_array()) {
                    let texts: Vec<String> = entries.iter()
                        .filter_map(|entry| {
                            // Try common text field names for each entry
                            for text_field in ["text", "content", "translation", "value"] {
                                if let Some(text) = entry.get(text_field).and_then(|t| t.as_str()) {
                                    return Some(text.to_string());
                                }
                            }
                            None
                        })
                        .collect();
                    
                    if !texts.is_empty() {
                        return Ok(texts.join("\n\n"));
                    }
                }
            }
            
            // Last resort: check for any string in the JSON structure
            if let Some(content) = json.as_str() {
                return Ok(content.to_string());
            }
        }
    }
    
    // If we've tried everything and couldn't find a clear translation,
    // return the original response with minimal cleaning
    let cleaned_response = response
        .trim()
        .replace("```json", "")
        .replace("```", "");
        
    Ok(cleaned_response)
}

impl TranslationService {
    /// New translation service with the specified configuration
    pub fn new(config: TranslationConfig) -> Result<Self> {
        // Match on the provider configuration and create the appropriate implementation
        let provider = match config.provider {
            ConfigTranslationProvider::Ollama => {
                // Parse the Ollama endpoint URL
                let (host, port) = parse_endpoint(&config.ollama.endpoint)?;
                
                // Create Ollama client
                let client = Ollama::new(host, port);
                
                TranslationProviderImpl::Ollama {
                    client,
                }
            },
            
            ConfigTranslationProvider::OpenAI => {
                // Create a regular reqwest client for OpenAI
                let client = Client::new();
                
                TranslationProviderImpl::OpenAI {
                    client,
                }
            },
            
            ConfigTranslationProvider::Anthropic => {
                // Create a regular reqwest client for Anthropic
                let client = Client::new();
                
                TranslationProviderImpl::Anthropic {
                    client,
                }
            },
        };
        
        Ok(Self {
            provider,
            config,
        })
    }
    
    /// Test connection to the translation service
    pub async fn test_connection(&self, source_language: &str, target_language: &str) -> Result<()> {
        match &self.provider {
            TranslationProviderImpl::Ollama { client } => {
                // Test if Ollama is available by sending a simple request
                // Use a small prompt to just test connectivity
                let prompt = "Hello";
                
                // Prepare request for Ollama
                let request = GenerationRequest::new(
                    self.config.ollama.model.clone(),
                    prompt,
                )
                .system("You are a helpful assistant.");
                
                // Try to generate a response from Ollama
                match client.generate(request).await {
                    Ok(_) => {
                        Ok(())
                    },
                    Err(e) => {
                        error!(" Ollama connection failed: {}", e);
                        Err(anyhow::anyhow!("Ollama connection test failed: {}", e))
                    }
                }
            },
            
            TranslationProviderImpl::OpenAI { client } => {
                // Determine endpoint based on whether we're using Azure or direct OpenAI
                let endpoint = if self.config.openai.endpoint.is_empty() {
                    "https://api.openai.com/v1/chat/completions".to_string()
                } else {
                    format!("{}/chat/completions", self.config.openai.endpoint.trim_end_matches('/'))
                };
                
                // Set up basic API request to test connectivity
                let request = OpenAIRequest {
                    model: self.config.openai.model.clone(),
                    messages: vec![
                        OpenAIMessage {
                            role: "user".to_string(),
                            content: "Hello".to_string(),
                        }
                    ],
                    temperature: Some(0.0),  // Use deterministic output for test
                    max_tokens: Some(10),    // Minimal response for connection test
                    top_p: Some(1.0),        // No filtering for test
                    frequency_penalty: None,
                    presence_penalty: None,
                    response_format: None,
                    user: None,
                };
                
                // Send a test request to the OpenAI API
                let response = client.post(&endpoint)
                    .header("Authorization", format!("Bearer {}", self.config.openai.api_key))
                    .json(&request)
                    .send()
                    .await;
                
                match response {
                    Ok(res) => {
                        if res.status().is_success() {
                            Ok(())
                        } else {
                            let status = res.status();
                            let error_text = res.text().await.unwrap_or_else(|_| "Unknown error".to_string());
                            error!("OpenAI connection failed: {} - {}", status, error_text);
                            Err(anyhow::anyhow!("OpenAI connection test failed. Status: {}, Error: {}", status, error_text))
                        }
                    },
                    Err(e) => {
                        error!("OpenAI connection failed: {}", e);
                        Err(anyhow::anyhow!("OpenAI connection test failed: {}", e))
                    }
                }
            },
            TranslationProviderImpl::Anthropic { client } => {
                let endpoint = if self.config.anthropic.endpoint.is_empty() {
                    "https://api.anthropic.com/v1/messages".to_string()
                } else {
                    format!("{}/v1/messages", self.config.anthropic.endpoint.trim_end_matches('/'))
                };
                
                let messages = vec![
                    AnthropicMessage { 
                        role: "user".to_string(), 
                        content: "Hello, this is a connection test.".to_string() 
                    }
                ];
                
                let request = AnthropicRequest {
                    model: self.config.anthropic.model.clone(),
                    system: Some("You are a helpful assistant.".to_string()),
                    messages,
                    temperature: Some(0.0), // Deterministic for connection test
                    max_tokens: 10, // Small response for test
                    top_p: None,
                    top_k: None,
                };
                
                let response = client
                    .post(&endpoint)
                    .header("x-api-key", &self.config.anthropic.api_key)
                    .header("anthropic-version", "2023-06-01")
                    .header("Content-Type", "application/json")
                    .json(&request)
                    .timeout(Duration::from_secs(10))
                    .send()
                    .await;
                
                match response {
                    Ok(res) => {
                        if res.status().is_success() {
                            Ok(())
                        } else {
                            let status = res.status();
                            let error_text = res.text().await.unwrap_or_else(|_| "Unknown error".to_string());
                            error!(" Anthropic connection failed: {} - {}", status, error_text);
                            Err(anyhow::anyhow!("Anthropic connection test failed. Status: {}, Error: {}", status, error_text))
                        }
                    },
                    Err(e) => {
                        error!(" Anthropic connection failed: {}", e);
                        Err(anyhow::anyhow!("Anthropic connection test failed: {}", e))
                    }
                }
            }
        }
    }
    
    /// Run a full translation test using a sample text
    pub async fn test_translation(&self, source_language: &str, target_language: &str) -> Result<String> {        
        // Try a simple translation test
        let sample = "Hello, world!";
        let translated = self.translate_text(sample, source_language, target_language).await?;
        
        Ok(translated)
    }
    
    /// Translate a single text
    async fn translate_text(&self, text: &str, source_language: &str, target_language: &str) -> Result<String> {
        match &self.provider {
            TranslationProviderImpl::Ollama { client } => {
                // Create prompt for translation more efficiently
                let mut prompt = String::with_capacity(text.len() + 100);
                prompt.push_str("Translate the following text from ");
                prompt.push_str(source_language);
                prompt.push_str(" to ");
                prompt.push_str(target_language);
                prompt.push_str(":\n\n");
                prompt.push_str(text);
                
                // Replace placeholders in the system prompt template efficiently
                let system_prompt = self.config.common.system_prompt
                    .replace("{source_language}", source_language)
                    .replace("{target_language}", target_language);
                
                // Create a request with system prompt
                let request = GenerationRequest::new(
                    self.config.ollama.model.clone(),
                    prompt
                )
                .system(system_prompt);
                
                // Use a timeout for the request
                let response_future = client.generate(request);
                
                // Set timeout based on configuration
                let timeout_duration = Duration::from_secs(self.config.ollama.timeout_secs);
                
                // Execute with timeout
                let response = tokio::time::timeout(timeout_duration, response_future)
                    .await
                    .map_err(|_| anyhow::anyhow!("Ollama request timed out after {} seconds", self.config.ollama.timeout_secs))?
                    .map_err(|e| anyhow::anyhow!("Failed to generate translation with Ollama: {}", e))?;
                
                Ok(response.response)
            },
            TranslationProviderImpl::OpenAI { client } => {
                // Create the chat request url efficiently
                let url = if self.config.openai.endpoint.is_empty() {
                    "https://api.openai.com/v1/chat/completions".to_string()
                } else {
                    let mut url = self.config.openai.endpoint.clone();
                    if url.ends_with('/') {
                        url.truncate(url.len() - 1);
                    }
                    url.push_str("/chat/completions");
                    url
                };
                
                // Replace placeholders in the system prompt template
                let system_prompt = self.config.common.system_prompt
                    .replace("{source_language}", source_language)
                    .replace("{target_language}", target_language);
                
                // Build messages efficiently
                let messages = vec![
                    OpenAIMessage {
                        role: "system".to_string(),
                        content: system_prompt,
                    },
                    OpenAIMessage {
                        role: "user".to_string(),
                        content: text.to_string(),
                    },
                ];
                
                let request = OpenAIRequest {
                    model: self.config.openai.model.clone(),
                    messages,
                    temperature: Some(0.3), // Lower temperature for more deterministic translation
                    max_tokens: Some((self.config.openai.max_chars_per_request * 2) as u32), // Estimate max tokens based on chars
                    top_p: Some(0.95), // Added top_p for better sampling quality 
                    frequency_penalty: Some(0.0), // Neutral setting for translation
                    presence_penalty: Some(0.0), // Neutral setting for translation
                    response_format: None, // Plain text format for translation
                    user: None, // Optional user tracking ID
                };
                
                // Create authorization header once
                let auth_bearer = format!("Bearer {}", self.config.openai.api_key);
                
                let response = client.post(&url)
                    .json(&request)
                    .header("Authorization", &auth_bearer)
                    .header("Content-Type", "application/json")
                    .timeout(Duration::from_secs(self.config.openai.timeout_secs))
                    .send()
                    .await
                    .map_err(|e| anyhow::anyhow!("Failed to send request to OpenAI API: {}", e))?;
                
                if !response.status().is_success() {
                    let status = response.status();
                    let error_text = response.text().await
                        .unwrap_or_else(|_| "Unknown error".to_string());
                    return Err(anyhow::anyhow!("OpenAI API error ({}): {}", status, error_text));
                }
                
                let data: OpenAIResponse = response.json().await
                    .context("Failed to parse OpenAI API response")?;
                
                if let Some(choice) = data.choices.first() {
                    Ok(choice.message.content.clone())
                } else {
                    Err(anyhow::anyhow!("No translation result in the API response"))
                }
            },
            TranslationProviderImpl::Anthropic { client } => {
                // Create the chat request url efficiently
                let url = if self.config.anthropic.endpoint.is_empty() {
                    "https://api.anthropic.com/v1/messages".to_string()
                } else {
                    let mut url = self.config.anthropic.endpoint.clone();
                    if url.ends_with('/') {
                        url.truncate(url.len() - 1);
                    }
                    url.push_str("/v1/messages");
                    url
                };
                
                // Replace placeholders in the system prompt template
                let system_prompt = self.config.common.system_prompt
                    .replace("{source_language}", source_language)
                    .replace("{target_language}", target_language);
                
                let request = AnthropicRequest {
                    model: self.config.anthropic.model.clone(),
                    system: Some(system_prompt),
                    messages: vec![
                        AnthropicMessage {
                            role: "user".to_string(),
                            content: text.to_string(),
                        },
                    ],
                    temperature: Some(0.3), // Lower temperature for more deterministic translation
                    max_tokens: (self.config.anthropic.max_chars_per_request * 2) as u32, // Estimate max tokens based on chars
                    top_p: Some(0.95), // Better quality sampling
                    top_k: Some(40), // Reasonable top_k value for quality
                };
                
                let response = client.post(&url)
                    .json(&request)
                    .header("x-api-key", &self.config.anthropic.api_key)
                    .header("anthropic-version", "2023-06-01")
                    .header("Content-Type", "application/json")
                    .timeout(Duration::from_secs(self.config.anthropic.timeout_secs))
                    .send()
                    .await
                    .map_err(|e| anyhow::anyhow!("Failed to send request to Anthropic API: {}", e))?;
                
                if !response.status().is_success() {
                    let status = response.status();
                    let error_text = response.text().await
                        .unwrap_or_else(|_| "Unknown error".to_string());
                    error!("Anthropic API error: {} - {}", status, error_text);
                    return Err(anyhow::anyhow!("Anthropic API error: {} - {}", status, error_text));
                }
                
                let data: AnthropicResponse = response.json().await
                    .context("Failed to parse Anthropic API response")?;
                
                if let Some(content) = data.content.first() {
                    Ok(content.text.clone())
                } else {
                    error!("No content found in Anthropic response");
                    Err(anyhow::anyhow!("No translation result in the API response"))
                }
            },
        }
    }
    
    /// Translate batches of subtitle entries
    ///
    /// This function takes batches of subtitle entries and translates them
    /// using the configured translation service
    pub async fn translate_batches(&self, 
                                  batches: &[Vec<SubtitleEntry>],
                                  source_language: &str, 
                                  target_language: &str,
                                  progress_callback: impl Fn(usize, usize) + Clone + Send + 'static) 
                                  -> Result<Vec<SubtitleEntry>> {
        let total_batches = batches.len();
        
        if total_batches == 0 {
            info!("No batches to translate, returning empty result");
            return Ok(Vec::new());
        }
        
        // Calculate the optimal number of concurrent requests based on provider
        let max_concurrent = self.config.optimal_concurrent_requests();
        // Rate limit delay is used in the batch processing task
        let _rate_limit_delay = Duration::from_millis(self.config.rate_limit_delay_ms());
        
        // Set up counters and progress tracking
        let completed = Arc::new(AtomicUsize::new(0));
        let total_entries: usize = batches.iter().map(|batch| batch.len()).sum();
        
        // Pre-allocate results vector with capacity for all entries
        let mut results: Vec<Vec<SubtitleEntry>> = Vec::with_capacity(total_batches);
        results.resize(total_batches, Vec::new());
        
        // Create a tokio semaphore to limit concurrent requests
        let semaphore = Arc::new(tokio::sync::Semaphore::new(max_concurrent));
        
        // Prepare tasks for each batch
        let mut tasks = Vec::with_capacity(total_batches);
        
        for (batch_idx, batch) in batches.iter().enumerate() {
            // Skip empty batches early
            if batch.is_empty() {
                info!("Skipping empty batch at index {}", batch_idx);
                let new_completed = completed.fetch_add(1, Ordering::SeqCst) + 1;
                progress_callback(new_completed, total_batches);
                continue;
            }
            
            // Clone necessary data for the task
            let batch = batch.to_vec();
            let source_language = source_language.to_string();
            let target_language = target_language.to_string();
            let semaphore = Arc::clone(&semaphore);
            let completed = Arc::clone(&completed);
            let retry_count = self.config.common.retry_count;
            let retry_backoff = self.config.common.retry_backoff_ms;
            let translation_service = self.clone();
            let progress_callback = progress_callback.clone();
            
            let task = tokio::spawn(async move {
                // Acquire semaphore permit to limit concurrent requests
                let _permit = semaphore.acquire().await.expect("Semaphore was closed");
                
                // Try multiple times if we get errors
                let mut attempts = 0;
                let mut last_error = None;
                
                while attempts <= retry_count {
                    // Apply exponential backoff delay for retries
                    if attempts > 0 {
                        let backoff_time = retry_backoff * (1 << (attempts - 1));
                        tokio::time::sleep(Duration::from_millis(backoff_time)).await;
                        // Only log retry attempts, not every batch processing
                        info!("Retry {} for batch {} (waiting {}ms)", 
                            attempts, batch_idx + 1, backoff_time);
                    }
                    
                    // Create a structured prompt with clear delimitation
                    let mut prompt = format!(
                        "Translate the following subtitles from {} to {}. Maintain exact formatting including line breaks.\n\n",
                        source_language, target_language
                    );
                    
                    // Add a clear delimiter for each subtitle entry with unique ID
                    for (i, entry) in batch.iter().enumerate() {
                        prompt.push_str(&format!("--- SUBTITLE {:04} START ---\n", i + 1));
                        prompt.push_str(&entry.escape_text());
                        prompt.push_str(&format!("\n--- SUBTITLE {:04} END ---\n\n", i + 1));
                    }
                    
                    // Add explicit instructions for the response format
                    prompt.push_str("\nYour response must follow this exact format for each subtitle:\n");
                    prompt.push_str("--- SUBTITLE XXXX START ---\n<translated text>\n--- SUBTITLE XXXX END ---\n\n");
                    prompt.push_str("where XXXX is the original subtitle number (1-indexed, zero-padded to 4 digits).\n");
                    prompt.push_str("Translate ONLY the text between the delimiters, keeping all formatting intact.\n");
                    
                    // Translate the prompt - removed verbose logging about prompt size
                    match translation_service.translate_text(
                        &prompt, 
                        &source_language, 
                        &target_language
                    ).await {
                        Ok(translated_text) => {
                            // Create a vector to hold the translated entries, pre-populated with originals as fallback
                            let mut translated_batch = batch.clone();
                            
                            // Define a robust regex pattern to extract subtitle entries with their IDs
                            let subtitle_regex = Regex::new(
                                r"---\s*SUBTITLE\s+(\d+)\s*(?:START)?\s*---\s*\n((?:.|\n)*?)---\s*SUBTITLE\s+\d+\s*(?:END)?\s*---"
                            ).unwrap_or_else(|_| {
                                // Fallback to a simpler pattern if the main one fails to compile
                                Regex::new(r"---\s*SUBTITLE\s+(\d+).*?---\s*\n((?:.|\n)*?)(?:---|\z)").unwrap()
                            });
                            
                            // Extract and process each subtitle entry
                            for cap in subtitle_regex.captures_iter(&translated_text) {
                                if let (Some(idx_match), Some(text_match)) = (cap.get(1), cap.get(2)) {
                                    if let Ok(idx) = idx_match.as_str().parse::<usize>() {
                                        // Adjust to 0-based index (pattern is 1-based)
                                        let idx = idx.saturating_sub(1);
                                        
                                        // Only update if the index is valid
                                        if idx < translated_batch.len() {
                                            let text = text_match.as_str().trim();
                                            translated_batch[idx].text = SubtitleEntry::unescape_text(text);
                                        } else {
                                            warn!("Invalid subtitle index {} (max: {})", 
                                                idx, translated_batch.len() - 1);
                                        }
                                    }
                                }
                            }
                            
                            // Verify we have the correct number of non-empty translations
                            let empty_count = translated_batch.iter()
                                .filter(|entry| entry.text.trim().is_empty())
                                .count();
                                
                            if empty_count > 0 {
                                warn!("Found {} empty translations in batch {}", empty_count, batch_idx + 1);
                                // Don't fail - original entries are kept as fallback
                            }
                            
                            // Update progress
                            let new_completed = completed.fetch_add(1, Ordering::SeqCst) + 1;
                            progress_callback(new_completed, total_batches);
                                
                            return Ok((batch_idx, translated_batch));
                        },
                        Err(e) => {
                            warn!("Translation attempt {} failed for batch {}/{}: {}", 
                                attempts + 1, batch_idx + 1, total_batches, e);
                            
                            last_error = Some(e);
                            attempts += 1;
                        }
                    }
                }
                
                // If all attempts failed, return error with the last error
                error!("All {} attempts failed for batch {}/{}", 
                    retry_count + 1, batch_idx + 1, total_batches);
                
                Err(last_error.unwrap_or_else(|| anyhow!("Translation failed with unknown error")))
            });
            
            tasks.push(task);
        }
        
        // Wait for all tasks to complete - no need to log this
        let task_results = join_all(tasks).await;
        
        // Create a hashmap to store results in case tasks complete out of order
        let mut result_map: std::collections::HashMap<usize, Vec<SubtitleEntry>> = 
            std::collections::HashMap::with_capacity(total_batches);
        
        // Process results
        for (i, task_result) in task_results.into_iter().enumerate() {
            match task_result {
                Ok(Ok((batch_idx, batch_entries))) => {
                    // Store the entries by batch index
                    result_map.insert(batch_idx, batch_entries);
                },
                Ok(Err(e)) => {
                    error!("Task {} failed: {}", i, e);
                    return Err(e);
                },
                Err(e) => {
                    error!("Task {} panicked: {}", i, e);
                    return Err(anyhow!("Task panicked: {}", e));
                }
            }
        }
        
        // Flatten results in the correct order
        let mut flattened_results = Vec::with_capacity(total_entries);
        
        // Add entries in order from the result map
        for idx in 0..total_batches {
            if let Some(entries) = result_map.remove(&idx) {
                flattened_results.extend(entries);
            }
        }
        
        // Verify all entries were processed
        if flattened_results.len() != total_entries {
            warn!("Entry count mismatch: expected {}, got {}", 
                total_entries, flattened_results.len());
                
            // If there are missing entries, we should at least not return fewer entries
            // than we started with - though this would indicate a serious error
            if flattened_results.len() < total_entries {
                error!("Missing {} entries after translation! Results may be incomplete.", 
                    total_entries - flattened_results.len());
            }
        }
                
        Ok(flattened_results)
    }
}

impl Clone for TranslationService {
    fn clone(&self) -> Self {
        // Clone the config only once
        let config = self.config.clone();
        
        // Create the appropriate provider based on the type
        let provider = match &self.provider {
            TranslationProviderImpl::Ollama { client: _ } => {
                // Ollama client isn't Clone, so we create a new one
                let (host, port) = parse_endpoint(&config.ollama.endpoint)
                    .expect("Failed to parse endpoint URL during clone");
                
                let client = Ollama::new(host, port);
                
                TranslationProviderImpl::Ollama {
                    client,
                }
            },
            TranslationProviderImpl::OpenAI { client: _ } => {
                TranslationProviderImpl::OpenAI {
                    client: Client::new(),
                }
            },
            TranslationProviderImpl::Anthropic { client: _ } => {
                TranslationProviderImpl::Anthropic {
                    client: Client::new(),
                }
            },
        };
        
        Self {
            provider,
            config,
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::app_config::{TranslationConfig, OllamaConfig, OpenAIConfig, AnthropicConfig, TranslationCommonConfig};
    use crate::app_config::TranslationProvider as ConfigTranslationProvider;

    fn get_test_config() -> TranslationConfig {
        TranslationConfig {
            provider: ConfigTranslationProvider::Ollama,
            ollama: OllamaConfig {
                model: "llama2".to_string(),
                endpoint: "http://localhost:11434".to_string(),
                concurrent_requests: 2,
                max_chars_per_request: 1000,
                timeout_secs: 30,
            },
            openai: OpenAIConfig {
                model: "gpt-3.5-turbo".to_string(),
                api_key: "test_key".to_string(),
                endpoint: "https://api.openai.com/v1".to_string(),
                concurrent_requests: 2,
                max_chars_per_request: 1000,
                timeout_secs: 30,
            },
            anthropic: AnthropicConfig {
                model: "claude-3-haiku".to_string(),
                api_key: "test_key".to_string(),
                endpoint: "https://api.anthropic.com".to_string(),
                concurrent_requests: 2,
                max_chars_per_request: 1000,
                timeout_secs: 30,
            },
            common: TranslationCommonConfig {
                system_prompt: "You are a professional translator. Translate from {source_language} to {target_language}.".to_string(),
                rate_limit_delay_ms: 100,
                retry_count: 2,
                retry_backoff_ms: 500,
            },
        }
    }

    #[test]
    fn test_translation_service_creation() {
        let config = get_test_config();
        let service = TranslationService::new(config);
        assert!(service.is_ok());
    }
} 